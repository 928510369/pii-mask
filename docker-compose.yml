vllm:
    image: vllm/vllm-openai:latest
    profiles:
      - gpu
    expose:
      - "8001"
    environment:
      # 显式告诉 vLLM 忽略 FA2，强制使用兼容 T4 的 kernels
      - VLLM_USE_XFORMERS=1
      # 某些情况下防止 NCCL 在旧卡上尝试 P2P 导致崩溃
      - NCCL_P2P_DISABLE=1
    volumes:
      - vllm-cache:/root/.cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: [
      "--host", "0.0.0.0",
      "--port", "8001",
      "--model", "Qwen/Qwen3-0.6B",
      "--max-num-seqs", "8",
      "--gpu-memory-utilization", "0.8",
      "--attn-backend", "xformers",  # 必须：T4 不支持默认的 FlashAttention-2
      "--enforce-eager",             # 建议：防止 torch.compile 在 16G 显存上溢出
      "--max-model-len", "4096",     # 建议：限制长度以保护 T4 有限的显存
      "--trust-remote-code"          # Qwen 模型通常需要此参数
    ]
    restart: unless-stopped